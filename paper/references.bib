
@article{mohey_enhancement_2016,
	title = {Enhancement {Bag}-of-{Words} {Model} for {Solving} the {Challenges} of {Sentiment} {Analysis}},
	volume = {7},
	issn = {21565570, 2158107X},
	url = {http://thesai.org/Publications/ViewPaper?Volume=7&Issue=1&Code=ijacsa&SerialNo=34},
	doi = {10.14569/IJACSA.2016.070134},
	abstract = {Sentiment analysis is a branch of natural language processing, or machine learning methods. It becomes one of the most important sources in decision making. It can extract, identify, evaluate or otherwise characterizes from the online sentiments reviews. Although Bag-Of-Words model is the most widely used technique for sentiment analysis, it has two major weaknesses: using a manual evaluation for a lexicon in determining the evaluation of words and analyzing sentiments with low accuracy because of neglecting the language grammar effects of the words and ignore semantics of the words. In this paper, we propose a new technique to evaluate online sentiments in one topic domain and produce a solution for some significant sentiment analysis challenges that improves the accuracy of sentiment analysis performed. The proposed technique relies on the enhancement bag-of-words model for evaluating sentiment polarity and score automatically by using the words weight instead of term frequency. This technique also can classify the reviews based on features and keywords of the scientific topic domain. This paper introduces solutions for essential sentiment analysis challenges that are suitable for the review structure. It also examines the effects by the proposed enhancement model to reach higher accuracy.},
	language = {en},
	number = {1},
	urldate = {2025-06-24},
	journal = {International Journal of Advanced Computer Science and Applications},
	author = {Mohey, Doaa},
	year = {2016},
	keywords = {/unread},
	file = {PDF:C\:\\Users\\Khor Kean Teng\\Zotero\\storage\\DJQZRSCK\\Mohey - 2016 - Enhancement Bag-of-Words Model for Solving the Challenges of Sentiment Analysis.pdf:application/pdf},
}

@misc{brownlee_how_2020,
	title = {How to {Develop} a {Deep} {Learning} {Bag}-of-{Words} {Model} for {Sentiment} {Analysis} ({Text} {Classification}) - {MachineLearningMastery}.com},
	shorttitle = {How to {Develop} a {Deep} {Learning} {Bag}-of-{Words} {Model} for {Sentiment} {Analysis} ({Text} {Classification})},
	url = {https://machinelearningmastery.com/deep-learning-bag-of-words-model-sentiment-analysis/},
	urldate = {2025-06-24},
	journal = {Machine Learning Mastery},
	author = {Brownlee, Jason},
	month = sep,
	year = {2020},
	keywords = {/unread},
	file = {How to Develop a Deep Learning Bag-of-Words Model for Sentiment Analysis (Text Classification) - MachineLearningMastery.com:C\:\\Users\\Khor Kean Teng\\Zotero\\storage\\NUVFSNIY\\deep-learning-bag-of-words-model-sentiment-analysis.html:text/html},
}

@misc{young_week_2024,
	title = {Week 3 {Session} 1 {Introduction} to {Language} {Models} and {N}-grams - {Introduction} to {NLP} and {LLMs} 2024},
	shorttitle = {Week 3 {Session} 1 {Introduction} to {Language} {Models} and {N}-grams},
	url = {https://nlp2024.jeju.ai/en/week03/session1.html},
	urldate = {2025-06-24},
	journal = {Introduction to NLP and LLMs 2024},
	author = {Young, Joon Lee},
	month = nov,
	year = {2024},
	keywords = {/unread},
	file = {Week 3 Session 1\: Introduction to Language Models and N-grams - Introduction to NLP and LLMs 2024:C\:\\Users\\Khor Kean Teng\\Zotero\\storage\\GK54BQCH\\session1.html:text/html},
}

@article{keselji_speech_2025,
	title = {Speech and {Language} {Processing} (second edition) {Daniel} {Jurafsky} and {James} {H}. {Martin} ({Stanford} {University} and {University} of {Colorado} at {Boulder}) {Pearson} {Prentice} {Hall}, 2009, {ISBN} 978-0-13-187321-6},
	url = {https://www.researchgate.net/publication/220355425_Speech_and_Language_Processing_second_edition_Daniel_Jurafsky_and_James_H_Martin_Stanford_University_and_University_of_Colorado_at_Boulder_Pearson_Prentice_Hall_2009_xxxi988_pp_hardbound_ISBN_978-0-13},
	doi = {10.1162/coli.B09-001},
	abstract = {PDF {\textbar} On Sep 1, 2009, Vlado Keselj published Speech and Language Processing (second edition) Daniel Jurafsky and James H. Martin (Stanford University and University of Colorado at Boulder) Pearson Prentice Hall, 2009, xxxi+988 pp; hardbound, ISBN 978-0-13-187321-6, \$115.00 {\textbar} Find, read and cite all the research you need on ResearchGate},
	language = {en},
	urldate = {2025-06-24},
	journal = {ResearchGate},
	author = {Keselji, Valdo},
	month = feb,
	year = {2025},
	keywords = {/unread},
	file = {Snapshot:C\:\\Users\\Khor Kean Teng\\Zotero\\storage\\K2HZADR4\\220355425_Speech_and_Language_Processing_second_edition_Daniel_Jurafsky_and_James_H_Martin_Stan.html:text/html;フルテキスト:C\:\\Users\\Khor Kean Teng\\Zotero\\storage\\NEKAJI7J\\2025 - (PDF) Speech and Language Processing (second edition) Daniel Jurafsky and James H. Martin (Stanford.pdf:application/pdf},
}

@article{bengio_neural_2003,
	title = {A {Neural} {Probabilistic} {Language} {Model}},
	url = {https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf},
	abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difﬁcult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to ﬁght the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a signiﬁcant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach signiﬁcantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
	language = {en},
	author = {Bengio, Yoshua and Ducharme, Réjean and Vincent, Pascal and Jauvin, Christian},
	year = {2003},
	keywords = {/unread},
	file = {PDF:C\:\\Users\\Khor Kean Teng\\Zotero\\storage\\NICQDBG5\\Bengio et al. - A Neural Probabilistic Language Model.pdf:application/pdf},
}

@misc{huang2020tabtransformertabulardatamodeling,
      title={TabTransformer: Tabular Data Modeling Using Contextual Embeddings}, 
      author={Xin Huang and Ashish Khetan and Milan Cvitkovic and Zohar Karnin},
      year={2020},
      eprint={2012.06678},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2012.06678}, 
}

@misc{dasgupta_hitgram_2024,
	title = {{HITgram}: {A} {Platform} for {Experimenting} with n-gram {Language} {Models}},
	shorttitle = {{HITgram}},
	url = {http://arxiv.org/abs/2412.10717},
	doi = {10.48550/arXiv.2412.10717},
	abstract = {Large language models (LLMs) are powerful but resource intensive, limiting accessibility. HITgram addresses this gap by offering a lightweight platform for n-gram model experimentation, ideal for resource-constrained environments. It supports unigrams to 4-grams and incorporates features like context sensitive weighting, Laplace smoothing, and dynamic corpus management to e-hance prediction accuracy, even for unseen word sequences. Experiments demonstrate HITgram's efficiency, achieving 50,000 tokens/second and generating 2-grams from a 320MB corpus in 62 seconds. HITgram scales efficiently, constructing 4-grams from a 1GB file in under 298 seconds on an 8 GB RAM system. Planned enhancements include multilingual support, advanced smoothing, parallel processing, and model saving, further broadening its utility.},
	urldate = {2025-06-24},
	publisher = {arXiv},
	author = {Dasgupta, Shibaranjani and Maity, Chandan and Mukherjee, Somdip and Singh, Rohan and Dutta, Diptendu and Jana, Debasish},
	month = dec,
	year = {2024},
	note = {arXiv:2412.10717 [cs]
TLDR: HITgram is a lightweight platform for n-gram model experimentation that supports unigrams to 4-grams and incorporates features like context sensitive weighting, Laplace smoothing, and dynamic corpus management to e-hance prediction accuracy, even for unseen word sequences.},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Full Text PDF:C\:\\Users\\Khor Kean Teng\\Zotero\\storage\\42JYBK3F\\Dasgupta et al. - 2024 - HITgram A Platform for Experimenting with n-gram Language Models.pdf:application/pdf;Snapshot:C\:\\Users\\Khor Kean Teng\\Zotero\\storage\\HX7KLPM5\\2412.html:text/html},
}

@misc{kaplan_scaling_2020,
	title = {Scaling {Laws} for {Neural} {Language} {Models}},
	url = {http://arxiv.org/abs/2001.08361},
	doi = {10.48550/arXiv.2001.08361},
	abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
	urldate = {2025-06-24},
	publisher = {arXiv},
	author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
	month = jan,
	year = {2020},
	note = {arXiv:2001.08361 [cs]},
	keywords = {/unread, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 19 pages, 15 figures},
	file = {Full Text PDF:C\:\\Users\\Khor Kean Teng\\Zotero\\storage\\WKRAWSVI\\Kaplan et al. - 2020 - Scaling Laws for Neural Language Models.pdf:application/pdf;Snapshot:C\:\\Users\\Khor Kean Teng\\Zotero\\storage\\7KDP5C8C\\2001.html:text/html},
}

@article{tay_long_2021,
	title = {{LONG} {RANGE} {ARENA}: {A} {BENCHMARK} {FOR} {EFFICIENT} {TRANSFORMERS}},
	abstract = {Transformers do not scale very well to long sequence lengths largely because of quadratic self-attention complexity. In the recent months, a wide spectrum of efﬁcient, fast Transformers have been proposed to tackle this problem, more often than not claiming superior or comparable model quality to vanilla Transformer models. To this date, there is no well-established consensus on how to evaluate this class of models. Moreover, inconsistent benchmarking on a wide spectrum of tasks and datasets makes it difﬁcult to assess relative model quality amongst many models. This paper proposes a systematic and uniﬁed benchmark, Long-Range Arena, speciﬁcally focused on evaluating model quality under long-context scenarios. Our benchmark is a suite of tasks consisting of sequences ranging from 1K to 16K tokens, encompassing a wide range of data types and modalities such as text, natural, synthetic images, and mathematical expressions requiring similarity, structural, and visual-spatial reasoning. We systematically evaluate ten well-established long-range Transformer models (Reformers, Linformers, Linear Transformers, Sinkhorn Transformers, Performers, Synthesizers, Sparse Transformers, and Longformers) on our newly proposed benchmark suite. Long-Range Arena paves the way towards better understanding this class of efﬁcient Transformer models, facilitates more research in this direction, and presents new challenging tasks to tackle.},
	language = {en},
	author = {Tay, Yi and Dehghani, Mostafa and Abnar, Samira and Shen, Yikang and Bahri, Dara and Pham, Philip and Rao, Jinfeng and Yang, Liu and Ruder, Sebastian and Metzler, Donald},
	year = {2021},
	keywords = {/unread},
	file = {PDF:C\:\\Users\\Khor Kean Teng\\Zotero\\storage\\FAVWH87P\\Tay et al. - 2021 - LONG RANGE ARENA A BENCHMARK FOR EFFICIENT TRANSFORMERS.pdf:application/pdf},
}

@misc{vaswani_attention_2023,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2025-06-24},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = aug,
	year = {2023},
	note = {arXiv:1706.03762 [cs]},
	keywords = {/unread, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 15 pages, 5 figures},
	file = {Full Text PDF:C\:\\Users\\Khor Kean Teng\\Zotero\\storage\\GKAPJDBV\\Vaswani et al. - 2023 - Attention Is All You Need.pdf:application/pdf;Snapshot:C\:\\Users\\Khor Kean Teng\\Zotero\\storage\\MY3TD4NQ\\1706.html:text/html},
}

@misc{wei_emergent_2022,
	title = {Emergent {Abilities} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2206.07682},
	doi = {10.48550/arXiv.2206.07682},
	abstract = {Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.},
	urldate = {2025-06-24},
	publisher = {arXiv},
	author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
	month = oct,
	year = {2022},
	note = {arXiv:2206.07682 [cs]
TLDR: This paper discusses an unpredictable phenomenon that is referred to as emergent abilities of large language models, an ability to be emergent if it is not present in smaller models but is present in larger models.},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Transactions on Machine Learning Research (TMLR), 2022},
	file = {Full Text PDF:C\:\\Users\\Khor Kean Teng\\Zotero\\storage\\3KCENDDL\\Wei et al. - 2022 - Emergent Abilities of Large Language Models.pdf:application/pdf;Snapshot:C\:\\Users\\Khor Kean Teng\\Zotero\\storage\\JIU8VP7F\\2206.html:text/html},
}

@misc{kadavath_language_2022,
	title = {Language {Models} ({Mostly}) {Know} {What} {They} {Know}},
	url = {http://arxiv.org/abs/2207.05221},
	doi = {10.48550/arXiv.2207.05221},
	abstract = {We study whether language models can evaluate the validity of their own claims and predict which questions they will be able to answer correctly. We first show that larger models are well-calibrated on diverse multiple choice and true/false questions when they are provided in the right format. Thus we can approach self-evaluation on open-ended sampling tasks by asking models to first propose answers, and then to evaluate the probability "P(True)" that their answers are correct. We find encouraging performance, calibration, and scaling for P(True) on a diverse array of tasks. Performance at self-evaluation further improves when we allow models to consider many of their own samples before predicting the validity of one specific possibility. Next, we investigate whether models can be trained to predict "P(IK)", the probability that "I know" the answer to a question, without reference to any particular proposed answer. Models perform well at predicting P(IK) and partially generalize across tasks, though they struggle with calibration of P(IK) on new tasks. The predicted P(IK) probabilities also increase appropriately in the presence of relevant source materials in the context, and in the presence of hints towards the solution of mathematical word problems. We hope these observations lay the groundwork for training more honest models, and for investigating how honesty generalizes to cases where models are trained on objectives other than the imitation of human writing.},
	urldate = {2025-06-24},
	publisher = {arXiv},
	author = {Kadavath, Saurav and Conerly, Tom and Askell, Amanda and Henighan, Tom and Drain, Dawn and Perez, Ethan and Schiefer, Nicholas and Hatfield-Dodds, Zac and DasSarma, Nova and Tran-Johnson, Eli and Johnston, Scott and El-Showk, Sheer and Jones, Andy and Elhage, Nelson and Hume, Tristan and Chen, Anna and Bai, Yuntao and Bowman, Sam and Fort, Stanislav and Ganguli, Deep and Hernandez, Danny and Jacobson, Josh and Kernion, Jackson and Kravec, Shauna and Lovitt, Liane and Ndousse, Kamal and Olsson, Catherine and Ringer, Sam and Amodei, Dario and Brown, Tom and Clark, Jack and Joseph, Nicholas and Mann, Ben and McCandlish, Sam and Olah, Chris and Kaplan, Jared},
	month = nov,
	year = {2022},
	note = {arXiv:2207.05221 [cs]
TLDR: It is shown that larger models are well-calibrated on diverse multiple choice and true/false questions when they are provided in the right format and investigated whether models can be trained to predict P(IK), the probability that "I know" the answer to a question.},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 23+17 pages; refs added, typos fixed},
	file = {Full Text PDF:C\:\\Users\\Khor Kean Teng\\Zotero\\storage\\5Z7CRXBF\\Kadavath et al. - 2022 - Language Models (Mostly) Know What They Know.pdf:application/pdf;Snapshot:C\:\\Users\\Khor Kean Teng\\Zotero\\storage\\5XZRYITP\\2207.html:text/html},
}

@misc{wei_inverse_2023,
	title = {Inverse scaling can become {U}-shaped},
	url = {http://arxiv.org/abs/2211.02011},
	doi = {10.48550/arXiv.2211.02011},
	abstract = {Scaling up language models has been empirically shown to improve performance on a wide range of downstream tasks. However, if we were to observe worse performance as a function of scale ("inverse scaling") on certain tasks, this would indicate that scaling can also encourage behaviors that are misaligned with human preferences. The Inverse Scaling Prize (McKenzie et al. 2022) identified eleven such inverse scaling tasks, evaluated on models of up to 280B parameters and up to 500 zettaFLOPs of training compute. This paper takes a closer look at these inverse scaling tasks. We evaluate models of up to 540B parameters, trained on five times more compute than those evaluated in the Inverse Scaling Prize. With this increased range of model sizes and training compute, only four out of the eleven tasks remain inverse scaling. Six out of the eleven tasks exhibit "U-shaped scaling", where performance decreases up to a certain size, and then increases again up to the largest model evaluated (the one remaining task displays positive scaling). In addition, we find that 1-shot examples and chain-of-thought can help mitigate undesirable scaling patterns even further. U-shaped scaling suggests that the inverse scaling trend observed in McKenzie et al. (2022) may not continue to hold for larger models, which we attribute to the presence of distractor tasks that only sufficiently large models can avoid.},
	urldate = {2025-06-24},
	publisher = {arXiv},
	author = {Wei, Jason and Kim, Najoung and Tay, Yi and Le, Quoc V.},
	month = may,
	year = {2023},
	note = {arXiv:2211.02011 [cs]
TLDR: U-shaped scaling suggests that the inverse scaling trend observed in McKenzie et al. (2022) may not continue to hold for larger models, which is attributed to the presence of distractor tasks that only sufficiently large models can avoid.},
	keywords = {/unread, Computer Science - Computation and Language},
	annote = {Comment: v5 includes a reframed discussion section and new chain-of-thought results for Round 2 tasks},
	file = {Full Text PDF:C\:\\Users\\Khor Kean Teng\\Zotero\\storage\\XU5Z7Y9E\\Wei et al. - 2023 - Inverse scaling can become U-shaped.pdf:application/pdf;Snapshot:C\:\\Users\\Khor Kean Teng\\Zotero\\storage\\TRFLZECN\\2211.html:text/html},
}

@article{fedus_switch_2022,
	title = {Switch {Transformers}: {Scaling} to {Trillion} {Parameter} {Models} with {Simple} and {Eﬃcient} {Sparsity}},
	language = {en},
	author = {Fedus, William and Zoph, Barret and Shazeer, Noam},
	year = {2022},
	file = {PDF:C\:\\Users\\Khor Kean Teng\\Zotero\\storage\\ZMH2LDN7\\Fedus et al. - Switch Transformers Scaling to Trillion Parameter Models with Simple and Eﬃcient Sparsity.pdf:application/pdf},
}

@misc{zhang_when_2020,
	title = {When {Do} {You} {Need} {Billions} of {Words} of {Pretraining} {Data}?},
	url = {http://arxiv.org/abs/2011.04946},
	doi = {10.48550/arXiv.2011.04946},
	abstract = {NLP is currently dominated by general-purpose pretrained language models like RoBERTa, which achieve strong performance on NLU tasks through pretraining on billions of words. But what exact knowledge or skills do Transformer LMs learn from large-scale pretraining that they cannot learn from less data? We adopt four probing methods---classifier probing, information-theoretic probing, unsupervised relative acceptability judgment, and fine-tuning on NLU tasks---and draw learning curves that track the growth of these different measures of linguistic ability with respect to pretraining data volume using the MiniBERTas, a group of RoBERTa models pretrained on 1M, 10M, 100M and 1B words. We find that LMs require only about 10M or 100M words to learn representations that reliably encode most syntactic and semantic features we test. A much larger quantity of data is needed in order to acquire enough commonsense knowledge and other skills required to master typical downstream NLU tasks. The results suggest that, while the ability to encode linguistic features is almost certainly necessary for language understanding, it is likely that other forms of knowledge are the major drivers of recent improvements in language understanding among large pretrained models.},
	urldate = {2025-06-24},
	publisher = {arXiv},
	author = {Zhang, Yian and Warstadt, Alex and Li, Haau-Sing and Bowman, Samuel R.},
	month = nov,
	year = {2020},
	note = {arXiv:2011.04946 [cs]},
	keywords = {/unread, Computer Science - Computation and Language},
	annote = {Comment: 10 pages, 6 figures},
	file = {Full Text PDF:C\:\\Users\\Khor Kean Teng\\Zotero\\storage\\JJ7DWNHY\\Zhang et al. - 2020 - When Do You Need Billions of Words of Pretraining Data.pdf:application/pdf;Snapshot:C\:\\Users\\Khor Kean Teng\\Zotero\\storage\\7Q4ZK23J\\2011.html:text/html},
}

@misc{rae_scaling_2022,
	title = {Scaling {Language} {Models}: {Methods}, {Analysis} \& {Insights} from {Training} {Gopher}},
	shorttitle = {Scaling {Language} {Models}},
	url = {http://arxiv.org/abs/2112.11446},
	doi = {10.48550/arXiv.2112.11446},
	abstract = {Language modelling provides a step towards intelligent communication systems by harnessing large repositories of written human knowledge to better predict and understand the world. In this paper, we present an analysis of Transformer-based language model performance across a wide range of model scales -- from models with tens of millions of parameters up to a 280 billion parameter model called Gopher. These models are evaluated on 152 diverse tasks, achieving state-of-the-art performance across the majority. Gains from scale are largest in areas such as reading comprehension, fact-checking, and the identification of toxic language, but logical and mathematical reasoning see less benefit. We provide a holistic analysis of the training dataset and model's behaviour, covering the intersection of model scale with bias and toxicity. Finally we discuss the application of language models to AI safety and the mitigation of downstream harms.},
	urldate = {2025-06-24},
	publisher = {arXiv},
	author = {Rae, Jack W. and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and Rutherford, Eliza and Hennigan, Tom and Menick, Jacob and Cassirer, Albin and Powell, Richard and Driessche, George van den and Hendricks, Lisa Anne and Rauh, Maribeth and Huang, Po-Sen and Glaese, Amelia and Welbl, Johannes and Dathathri, Sumanth and Huang, Saffron and Uesato, Jonathan and Mellor, John and Higgins, Irina and Creswell, Antonia and McAleese, Nat and Wu, Amy and Elsen, Erich and Jayakumar, Siddhant and Buchatskaya, Elena and Budden, David and Sutherland, Esme and Simonyan, Karen and Paganini, Michela and Sifre, Laurent and Martens, Lena and Li, Xiang Lorraine and Kuncoro, Adhiguna and Nematzadeh, Aida and Gribovskaya, Elena and Donato, Domenic and Lazaridou, Angeliki and Mensch, Arthur and Lespiau, Jean-Baptiste and Tsimpoukelli, Maria and Grigorev, Nikolai and Fritz, Doug and Sottiaux, Thibault and Pajarskas, Mantas and Pohlen, Toby and Gong, Zhitao and Toyama, Daniel and d'Autume, Cyprien de Masson and Li, Yujia and Terzi, Tayfun and Mikulik, Vladimir and Babuschkin, Igor and Clark, Aidan and Casas, Diego de Las and Guy, Aurelia and Jones, Chris and Bradbury, James and Johnson, Matthew and Hechtman, Blake and Weidinger, Laura and Gabriel, Iason and Isaac, William and Lockhart, Ed and Osindero, Simon and Rimell, Laura and Dyer, Chris and Vinyals, Oriol and Ayoub, Kareem and Stanway, Jeff and Bennett, Lorrayne and Hassabis, Demis and Kavukcuoglu, Koray and Irving, Geoffrey},
	month = jan,
	year = {2022},
	note = {arXiv:2112.11446 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: 120 pages},
	file = {Full Text PDF:C\:\\Users\\Khor Kean Teng\\Zotero\\storage\\KU5XVVYG\\Rae et al. - 2022 - Scaling Language Models Methods, Analysis & Insights from Training Gopher.pdf:application/pdf;Snapshot:C\:\\Users\\Khor Kean Teng\\Zotero\\storage\\DYMGFVEB\\2112.html:text/html},
}

@misc{lee_deduplicating_2022,
	title = {Deduplicating {Training} {Data} {Makes} {Language} {Models} {Better}},
	url = {http://arxiv.org/abs/2107.06499},
	doi = {10.48550/arXiv.2107.06499},
	abstract = {We find that existing language modeling datasets contain many near-duplicate examples and long repetitive substrings. As a result, over 1\% of the unprompted output of language models trained on these datasets is copied verbatim from the training data. We develop two tools that allow us to deduplicate training datasets -- for example removing from C4 a single 61 word English sentence that is repeated over 60,000 times. Deduplication allows us to train models that emit memorized text ten times less frequently and require fewer train steps to achieve the same or better accuracy. We can also reduce train-test overlap, which affects over 4\% of the validation set of standard datasets, thus allowing for more accurate evaluation. We release code for reproducing our work and performing dataset deduplication at https://github.com/google-research/deduplicate-text-datasets.},
	urldate = {2025-06-24},
	publisher = {arXiv},
	author = {Lee, Katherine and Ippolito, Daphne and Nystrom, Andrew and Zhang, Chiyuan and Eck, Douglas and Callison-Burch, Chris and Carlini, Nicholas},
	month = mar,
	year = {2022},
	note = {arXiv:2107.06499 [cs]},
	keywords = {/unread, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Accepted to ACL 2022},
	file = {Full Text PDF:C\:\\Users\\Khor Kean Teng\\Zotero\\storage\\SKSNAN5I\\Lee et al. - 2022 - Deduplicating Training Data Makes Language Models Better.pdf:application/pdf;Snapshot:C\:\\Users\\Khor Kean Teng\\Zotero\\storage\\5W8L43U5\\2107.html:text/html},
}

@misc{hoffmann_training_2022,
	title = {Training {Compute}-{Optimal} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2203.15556},
	doi = {10.48550/arXiv.2203.15556},
	abstract = {We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4\${\textbackslash}times\$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the MMLU benchmark, greater than a 7\% improvement over Gopher.},
	urldate = {2025-06-24},
	publisher = {arXiv},
	author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and Driessche, George van den and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Rae, Jack W. and Vinyals, Oriol and Sifre, Laurent},
	month = mar,
	year = {2022},
	note = {arXiv:2203.15556 [cs]},
	keywords = {/unread, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Full Text PDF:C\:\\Users\\Khor Kean Teng\\Zotero\\storage\\D6HCBM4W\\Hoffmann et al. - 2022 - Training Compute-Optimal Large Language Models.pdf:application/pdf;Snapshot:C\:\\Users\\Khor Kean Teng\\Zotero\\storage\\VW42APTE\\2203.html:text/html},
}

@misc{chung_scaling_2022,
	title = {Scaling {Instruction}-{Finetuned} {Language} {Models}},
	url = {http://arxiv.org/abs/2210.11416},
	doi = {10.48550/arXiv.2210.11416},
	abstract = {Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM 540B by a large margin (+9.4\% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2\% on five-shot MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.},
	urldate = {2025-06-24},
	publisher = {arXiv},
	author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Castro-Ros, Alex and Pellat, Marie and Robinson, Kevin and Valter, Dasha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},
	month = dec,
	year = {2022},
	note = {arXiv:2210.11416 [cs]
TLDR: It is found that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups, and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation).},
	keywords = {/unread, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Public checkpoints: https://huggingface.co/docs/transformers/model\_doc/flan-t5},
	file = {Full Text PDF:C\:\\Users\\Khor Kean Teng\\Zotero\\storage\\H8WRXQBE\\Chung et al. - 2022 - Scaling Instruction-Finetuned Language Models.pdf:application/pdf;Snapshot:C\:\\Users\\Khor Kean Teng\\Zotero\\storage\\SGNXBCX6\\2210.html:text/html},
}

@misc{ouyang_training_2022,
	title = {Training language models to follow instructions with human feedback},
	url = {http://arxiv.org/abs/2203.02155},
	doi = {10.48550/arXiv.2203.02155},
	abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
	urldate = {2025-06-24},
	publisher = {arXiv},
	author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
	month = mar,
	year = {2022},
	note = {arXiv:2203.02155 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Full Text PDF:C\:\\Users\\Khor Kean Teng\\Zotero\\storage\\MRST8WG5\\Ouyang et al. - 2022 - Training language models to follow instructions with human feedback.pdf:application/pdf;Snapshot:C\:\\Users\\Khor Kean Teng\\Zotero\\storage\\2ZUDQT9I\\2203.html:text/html},
}

@misc{khandelwal_generalization_2020,
	title = {Generalization through {Memorization}: {Nearest} {Neighbor} {Language} {Models}},
	shorttitle = {Generalization through {Memorization}},
	url = {http://arxiv.org/abs/1911.00172},
	doi = {10.48550/arXiv.1911.00172},
	abstract = {We introduce \$k\$NN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a \$k\$-nearest neighbors (\$k\$NN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this augmentation to a strong Wikitext-103 LM, with neighbors drawn from the original training set, our \$k\$NN-LM achieves a new state-of-the-art perplexity of 15.79 - a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail.},
	urldate = {2025-06-24},
	publisher = {arXiv},
	author = {Khandelwal, Urvashi and Levy, Omer and Jurafsky, Dan and Zettlemoyer, Luke and Lewis, Mike},
	month = feb,
	year = {2020},
	note = {arXiv:1911.00172 [cs]},
	keywords = {/unread, Computer Science - Computation and Language},
	annote = {Comment: ICLR 2020},
	file = {Full Text PDF:C\:\\Users\\Khor Kean Teng\\Zotero\\storage\\6QA3P5DJ\\Khandelwal et al. - 2020 - Generalization through Memorization Nearest Neighbor Language Models.pdf:application/pdf;Snapshot:C\:\\Users\\Khor Kean Teng\\Zotero\\storage\\S4MQDET3\\1911.html:text/html},
}

@misc{borgeaud_improving_2021,
	title = {Improving language models by retrieving from trillions of tokens},
	url = {https://deepmind.google/discover/blog/improving-language-models-by-retrieving-from-trillions-of-tokens/},
	abstract = {We explore an alternate path for improving language models: we augment transformers with retrieval over a database of text passages including web pages, books, news and code. We call our method...},
	language = {en},
	urldate = {2025-06-24},
	journal = {Google DeepMind},
	author = {Borgeaud, Sebastian},
	month = dec,
	year = {2021},
	keywords = {/unread},
	file = {Snapshot:C\:\\Users\\Khor Kean Teng\\Zotero\\storage\\JB23RGYD\\improving-language-models-by-retrieving-from-trillions-of-tokens.html:text/html},
}

@misc{wei_better_2022,
	title = {Better {Language} {Models} {Without} {Massive} {Compute}},
	url = {https://research.google/blog/better-language-models-without-massive-compute/},
	abstract = {Posted by Jason Wei and Yi Tay, Research Scientists, Google Research, Brain Team In recent years, language models (LMs) have become more prominent ...},
	language = {en},
	urldate = {2025-06-24},
	journal = {Google Research},
	author = {Wei, Jason},
	month = nov,
	year = {2022},
	keywords = {/unread},
}

\subsection*{Part B}

\textit{Identify and explain TWO (2) key limitations of statistical language models. Demonstrate how modern neural approaches address these limitations by providing relevant examples.}

\begin{center}
  $\ast$~$\ast$~$\ast$
\end{center}

Statistical language models while foundational to natural language processing, possess inherent limitations that have paved the way for more advanced neural network-based approaches. Two key limitations of are data sparsity and their restricted ability to capture long-range dependencies and complex contextual information. Modern neural approaches, such as Recurrent Neural Networks (RNNs) and Transformer models, have made significant strides in addressing these shortcomings.

Considering data sparsity, as noted in n-gram models when estimating the probability of a word occurring given its preceding n-1 words. A major challenge arises when a specific n-gram (a sequence of n words) has never or rarely appeared in the training corpus. This leads to zero or unreliable probability estimates for those unseen or rare sequences, a problem known as data sparsity. As the value of 'n' (the context window) increases to capture more context, the number of possible n-grams grows exponentially, making data sparsity an even more pronounced issue. This means the model struggles to generalize to new or less frequent word combinations, negatively impacting its predictive accuracy \parencite{dasgupta_hitgram_2024}.

To overcome this issue, Modern neural architectures, particularly RNNs (and their variants like LSTMs) and Transformer models, are designed to better capture sequential information and long-range dependencies. For instance, RNNs possess a "memory" in the form of a hidden state that allows them to retain information from previous inputs in a sequence. As an RNN processes a sentence word by word, its hidden state is updated, theoretically allowing it to capture information from earlier words to influence predictions for later words. LSTMs (Long Short-Term Memory networks), a special type of RNN, were specifically designed to combat the vanishing gradient problem, enabling them to learn and remember information over longer sequences. In the sentence, \textbf{"Maya, which I confessed to earlier, is now blushing"}, an RNN/LSTM can theoretically maintain information about "Maya" across the intervening clause "which I confessed to earlier" to correctly predict that "is" (singular verb) agrees with "Maya".

Admittedly, modern state-of-the-art language models are based on the transformer architecture. Transformers have revolutionized the handling of long-range dependencies through a mechanism called self-attention. Unlike RNNs that process words sequentially, transformers can process all words in a sequence simultaneously. The self-attention mechanism allows the model to weigh the importance of all other words in the input sequence when encoding a particular word. This means that for any given word, the model can directly "attend" to and draw context from any other word in the sequence, regardless of its distance. In a long paragraph discussing a specific concept introduced at the beginning, a Transformer model can use its self-attention mechanism to directly link later mentions of pronouns or related terms back to the original concept, even if they are many sentences apart. This allows for a more holistic understanding of the text and more contextually relevant predictions or generations.

Above all, while statistical language models laid crucial groundwork, modern neural approaches like RNNs and especially Transformers have significantly advanced the field by effectively mitigating issues of data sparsity through learned representations (embeddings) and by capturing more extensive contextual information and long-range dependencies through architectural innovations like recurrent states and self-attention mechanisms.

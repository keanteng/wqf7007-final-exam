\section*{Question 1}

\textit{Explain TWO (2) shortcomings of Bag-of-Words (BoW) and TF-IDF features in sentiment analysis, supporting your explanation with relevant examples. In addition to Word Embeddings and Transformer-based methods, suggest at least TWO (2) other alternative approaches that can help overcome these limitations. Use linguistic examples to clarify your suggestions.}

\begin{center}
  $\ast$~$\ast$~$\ast$
\end{center}

BoW represents text as a vector of word frequencies, ignoring word order, while TF-IDF weights these frequencies by the rarity of words across documents. Both are widely used in sentiment analysis, which involves classifying text as positive, negative, or neutral. However, their simplicity can lead to significant shortcomings, particularly in capturing the nuances of human language.

One of the shortcomings of BoW and TF-IDF is loss of word order and context. For instance, BoW and TF-IDF treat text as an unordered collection, disregarding grammar and syntax. This is particularly problematic for sentiment analysis, where context is crucial. For instance, consider the sentences \textbf{"I like the person called Maya"} and \textbf{"I do not like the person called Maya."} In BoW, both might have similar word counts for \textbf{"like"} \textbf{"person"} etc., but the presence of \textbf{"not"} in the second sentence reverses the sentiment. In fact BoW and TF-IDF can misclassify negations without considering word order \parencite{mohey_enhancement_2016}. Another example is sarcasm or mixed sentiments, like "Oh great, another that's so bad." BoW might count "great" as positive, missing the ironic tone, which can lead to incorrect sentiment classification \parencite{brownlee_how_2020}.

Furthermore, BoW and TF-IDF struggles to capture semantic meaning. This is because they treat each word independently, without understanding semantic relationships such as synonyms, antonyms, or sentiment intensity. For example, "good" and "excellent" both convey positive sentiment, but "excellent" implies a stronger positive intensity. BoW would represent them as separate features, potentially leading to models that fail to generalize across similar meanings. This was identified as a major weakness in an academic paper, which noted that BoW ignores semantics, affecting sentiment analysis accuracy \parencite{mohey_enhancement_2016}. Another example is polysemy, where a word like "bank" can mean a financial institution (potentially neutral or negative in sentiment) or the side of a river (neutral). Without semantic understanding, BoW might misinterpret the context. Additionally, the high dimensionality of BoW vectors, often sparse due to many zero values, can make it harder for models to learn effectively \parencite{brownlee_how_2020}.

Some alternatives to overcome the identified shortcomings are n-grams and sentiment lexicons. Considering N-grams that involve using sequences of words, such as bigrams (two words) or trigrams (three words), they can capture local context and word order. This approach can address the loss of context by including phrases that carry sentiment, such as negations or intensifiers. For example, in the sentence \textbf{"Maya is not beautiful"}, the bigram \textbf{"not beautiful"} can be a feature that indicates negative sentiment, whereas in \textbf{"Maya is a beautiful person"}, the bigram \textbf{"beautiful person"} might indicate positive sentiment. Moreover, N-grams also help with sentiment intensity, as phrases like "very good" (bigram: "very good") can indicate stronger positivity compared to just "good." This approach is particularly useful for handling idiomatic expressions and negations, addressing both shortcomings identified earlier.

Alternatively, think about sentiment lexicons which are dictionaries that assign sentiment scores to words, often with positive, negative, or neutral labels, and sometimes including intensity (e.g., -1 to +1). They can incorporate semantic meaning by providing predefined sentiment for words, addressing the second shortcoming. For example, words like "happy," "joy," and "excellent" might have high positive scores, while "sad," "bad," and "terrible" have negative scores. In a sentence like "The service was excellent," "excellent" would contribute a strong positive score, improving sentiment classification. That is not all, some lexicons also handle negations and intensifiers through rules, such as reversing the score if "not" precedes a word. For instance, in "The service was not excellent," the lexicon might adjust the score of "excellent" to negative, addressing the context issue to some extent. 

To recap, consider the sentence \textbf{"I thought Maya was beautiful, but it was ugly"}. BoW might count \textbf{"beautiful"} and \textbf{"ugly"} potentially averaging to neutral, missing the overall negative sentiment due to the contrast. Using n-grams, bigrams like \textbf{"was beautiful"} and \textbf{"was ugly"} could be features, and with appropriate weighting, the model might capture the negative conclusion. Similarly, a sentiment lexicon would score "beautiful" positively and "not" negatively, and rules could prioritize the latter for final sentiment, addressing both shortcomings. In addition, "The acting was great, but the plot was terrible." N-grams could include "acting great" (positive) and "plot terrible" (negative), while a lexicon would score "great" highly positive and "terrible" highly negative, allowing for aspect-based sentiment analysis.

In a nutshell, BoW and TF-IDF's limitations in sentiment analysis stem from their inability to handle word order and semantic relationships, as evidenced by examples like negations and sentiment intensity. Alternatives like n-grams and sentiment lexicons offer practical solutions, improving context and semantic understanding, respectively. These approaches, supported by recent research, provide robust methods for enhancing sentiment analysis accuracy.

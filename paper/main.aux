\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\abx@aux@refcontext{apa/global//global/global/global}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{mohey_enhancement_2016}
\abx@aux@segm{0}{0}{mohey_enhancement_2016}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{brownlee_how_2020}
\abx@aux@segm{0}{0}{brownlee_how_2020}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{mohey_enhancement_2016}
\abx@aux@segm{0}{0}{mohey_enhancement_2016}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{brownlee_how_2020}
\abx@aux@segm{0}{0}{brownlee_how_2020}
\abx@aux@page{1}{1}
\abx@aux@page{2}{1}
\abx@aux@page{3}{1}
\abx@aux@page{4}{1}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{young_week_2024}
\abx@aux@segm{0}{0}{young_week_2024}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{keselji_speech_2025}
\abx@aux@segm{0}{0}{keselji_speech_2025}
\abx@aux@page{5}{2}
\abx@aux@page{6}{2}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{bengio_neural_2003}
\abx@aux@segm{0}{0}{bengio_neural_2003}
\abx@aux@page{7}{3}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{dasgupta_hitgram_2024}
\abx@aux@segm{0}{0}{dasgupta_hitgram_2024}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{kaplan_scaling_2020}
\abx@aux@segm{0}{0}{kaplan_scaling_2020}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{fedus_switch_2022}
\abx@aux@segm{0}{0}{fedus_switch_2022}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{hoffmann_training_2022}
\abx@aux@segm{0}{0}{hoffmann_training_2022}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{rae_scaling_2022}
\abx@aux@segm{0}{0}{rae_scaling_2022}
\abx@aux@page{8}{4}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{vaswani_attention_2023}
\abx@aux@segm{0}{0}{vaswani_attention_2023}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{kaplan_scaling_2020}
\abx@aux@segm{0}{0}{kaplan_scaling_2020}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Speed advantage of Switch Transformer. All models trained on 32 TPUv3 cores with equal FLOPs per example. For a xed amount of computation and training time, Switch Transformers signi cantly outperform the dense Transformer baseline. Our 64 expert Switch-Base model achieves the same quality in one-seventh the time of the T5-Base and continues to improve}}{5}{figure.1}\protected@file@percent }
\newlabel{fig:switch}{{1}{5}{Speed advantage of Switch Transformer. All models trained on 32 TPUv3 cores with equal FLOPs per example. For a xed amount of computation and training time, Switch Transformers signi cantly outperform the dense Transformer baseline. Our 64 expert Switch-Base model achieves the same quality in one-seventh the time of the T5-Base and continues to improve}{figure.1}{}}
\abx@aux@page{9}{5}
\abx@aux@page{10}{5}
\abx@aux@page{11}{5}
\abx@aux@page{12}{5}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces An overview of the percentage change in performance metric (higher is better) of Gopher versus state-of-the-art language model performance across 124 tasks. Each bar represents a task, here we clip the maximum relative improvement to 120\%. In total Gopher shows an improvement across 100 / 124. The best-published results include (175B) GPT-3, (178B) Jurassic-1, and (530B) Megatron-Turing NLG.}}{5}{figure.2}\protected@file@percent }
\newlabel{fig:gopher}{{2}{5}{An overview of the percentage change in performance metric (higher is better) of Gopher versus state-of-the-art language model performance across 124 tasks. Each bar represents a task, here we clip the maximum relative improvement to 120\%. In total Gopher shows an improvement across 100 / 124. The best-published results include (175B) GPT-3, (178B) Jurassic-1, and (530B) Megatron-Turing NLG}{figure.2}{}}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{wei_emergent_2022}
\abx@aux@segm{0}{0}{wei_emergent_2022}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{borgeaud_improving_2021}
\abx@aux@segm{0}{0}{borgeaud_improving_2021}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{khandelwal_generalization_2020}
\abx@aux@segm{0}{0}{khandelwal_generalization_2020}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{tay_long_2021}
\abx@aux@segm{0}{0}{tay_long_2021}
\abx@aux@cite{0}{chung_scaling_2022}
\abx@aux@segm{0}{0}{chung_scaling_2022}
\abx@aux@refcontext{apa/apasortcite//global/global/global}
\abx@aux@cite{0}{wei_emergent_2022}
\abx@aux@segm{0}{0}{wei_emergent_2022}
\abx@aux@page{13}{6}
\abx@aux@page{14}{6}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Model Scaling: Eight examples of emergence in the few-shot prompting setting. Each point is a separate model.The ability to perform a task via few-shot prompting is emergent when a language model achieves random performance until a certain scale,after which performance significantly increases to well-above random.Note that models that used more training compute also typically have more parameters—hence,we show ananalogous figure with number of model parameters instead of training FLOPs as the x-axis}}{6}{figure.3}\protected@file@percent }
\newlabel{fig:model_scaling}{{3}{6}{Model Scaling: Eight examples of emergence in the few-shot prompting setting. Each point is a separate model.The ability to perform a task via few-shot prompting is emergent when a language model achieves random performance until a certain scale,after which performance significantly increases to well-above random.Note that models that used more training compute also typically have more parameters—hence,we show ananalogous figure with number of model parameters instead of training FLOPs as the x-axis}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Augmented Prompting: Specialized prompting or finetuning methods can be emergent in that they do not have a positive effect until a certain model scale.}}{6}{figure.4}\protected@file@percent }
\newlabel{fig:augmendted_prompting}{{4}{6}{Augmented Prompting: Specialized prompting or finetuning methods can be emergent in that they do not have a positive effect until a certain model scale}{figure.4}{}}
\abx@aux@page{15}{6}
\abx@aux@page{16}{6}
\abx@aux@page{17}{6}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The RETRO model stays more on-topic than the baseline sample.Type image caption here (optional)}}{7}{figure.5}\protected@file@percent }
\newlabel{fig:retro}{{5}{7}{The RETRO model stays more on-topic than the baseline sample.Type image caption here (optional)}{figure.5}{}}
\abx@aux@page{18}{7}
\abx@aux@page{19}{7}
\abx@aux@page{20}{7}
\abx@aux@page{21}{7}
\abx@aux@page{22}{7}
\abx@aux@page{23}{7}
\abx@aux@page{24}{7}
\abx@aux@page{25}{7}
\abx@aux@page{26}{7}
\abx@aux@page{27}{7}
\abx@aux@page{28}{7}
\abx@aux@page{29}{7}
\abx@aux@page{30}{8}
\abx@aux@page{31}{8}
\abx@aux@page{32}{8}
\abx@aux@page{33}{8}
\abx@aux@page{34}{8}
\abx@aux@page{35}{8}
\abx@aux@page{36}{8}
\abx@aux@read@bbl@mdfivesum{2524179639C4B313CF320567946FF270}
\abx@aux@defaultrefcontext{0}{bengio_neural_2003}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{borgeaud_improving_2021}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{brownlee_how_2020}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{chung_scaling_2022}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{dasgupta_hitgram_2024}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{fedus_switch_2022}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{hoffmann_training_2022}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{kaplan_scaling_2020}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{keselji_speech_2025}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{khandelwal_generalization_2020}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{mohey_enhancement_2016}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{rae_scaling_2022}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{tay_long_2021}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{vaswani_attention_2023}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{wei_emergent_2022}{apa/global//global/global/global}
\abx@aux@defaultrefcontext{0}{young_week_2024}{apa/global//global/global/global}
\gdef \@abspage@last{8}
